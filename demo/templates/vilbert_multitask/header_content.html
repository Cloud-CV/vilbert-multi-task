<script>
    var task_data = "";
</script>

<div class="page-header">
    <h1 class="align-center">CloudCV: ViLBERT Multi-Task Demo</h1>
    <div class="fs-16 new-tab">
        <p>A single ViLBERT Multi-Task model can perform 8 different vision and language tasks learnt from 12 datasets!</p>
        <p>Datasets: <a href="https://visualqa.org/download.html">VQA v2</a>, <a href="https://cs.stanford.edu/people/dorarad/gqa/download.html">GQA</a>,
            <a href="https://visualgenome.org/">Visual Genome QA</a>, <a href="https://github.com/lichengunc/refer">RefCOCO</a>,
            <a href="https://github.com/lichengunc/refer">RefCOCO+</a>, <a href="https://github.com/lichengunc/refer">RefCOCOg</a>,
            <a href="https://github.com/yukezhu/visual7w-toolkit">Visual7W</a>, <a href="https://www.guesswhat.ai/">GuessWhat</a>,
            <a href="http://cocodataset.org/#download">COCO Retrieval</a>, <a href=" http://hockenmaier.cs.illinois.edu/DenotationGraph/">Flickr30k Retrieval</a>,
            <a href="https://github.com/necla-ml/SNLI-VE">SNLI-VE</a>, <a href="http://lil.nlp.cornell.edu/nlvr/">NLVR2</a>.</p>
        <p>More details about the ViLBERT Multi-Task paper can be found <a href="https://arxiv.org/pdf/1912.02315.pdf">here</a> along  with the <a href="https://github.com/facebookresearch/vilbert-multi-task"> code for model training</a>.</p>
        <p>Browsers currently supported by the demo: Google Chrome, Mozilla Firefox.
        </p>
    </div>
</div>